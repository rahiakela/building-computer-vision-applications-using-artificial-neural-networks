{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-detecting-objects-using-trained-models.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1a7Q6yJOGa8CdtKOcQClYW8yK7AygbfTt",
      "authorship_tag": "ABX9TyO67ILL0/uYjM9fG68VjJK7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/building-computer-vision-applications-using-artificial-neural-networks/blob/master/6-deep-learning-in-object-detection/2_detecting_objects_using_trained_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRwK1mdHyH4M"
      },
      "source": [
        "# Detecting Objects Using Trained Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ-LqT1PzCcj"
      },
      "source": [
        "As we learned before, model training is not a frequent activity and, when we have a reasonably good model (high accuracy or mAP), we may not need to retrain the model for as long as the model gives accurate predictions. Also, the model training is compute-intensive, and it takes several hours or days to train a good model even on GPUs. It is sometimes desirable and economical to train your computer vision models on the cloud and use GPUs. When the model is ready, download it to use locally in your computer or application server, which will use this model to detect objects in images.\n",
        "\n",
        "We will follow this high-level plan to develop our predictor:\n",
        "\n",
        "1. Install the TensorFlow models project\n",
        "2. Utilize the exported TensorFlow graph (exported model) to predict objects within new images that were not included in the training or test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLud-Bz7yJIh"
      },
      "source": [
        "## Installing TensorFlow’s models Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8S-jtxLOjAYm"
      },
      "source": [
        "The installation process of the TensorFlow models project is the same as we did on Google Colab. The difference may be in the Protobuf installation as it is platform-dependent software.\n",
        "\n",
        "Here is the full set of steps to install and configure the models project:\n",
        "\n",
        "1. First, let’s install a few necessary libraries that are needed to build\n",
        "and install the models project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pndeGsl_wSkC"
      },
      "source": [
        "%%shell\n",
        "pip install --user Cython\n",
        "pip install --user contextlib2\n",
        "pip install --user pillow\n",
        "pip install --user lxml\n",
        "pip install --user matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ve8QBP-pApA"
      },
      "source": [
        "2. Install Google’s Protobuf compiler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyDv55Kwo5o7"
      },
      "source": [
        "%%shell\n",
        "%tensorflow_version 1.x\n",
        "sudo apt-get install protobuf-compiler python-pil python-lxml python-tk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c_pO8mZpbtz"
      },
      "source": [
        "3. Clone the TensorFlow models project from GitHub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbSH4PNEpdYy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68c74366-5660-497a-b66e-6a8b681a7f70"
      },
      "source": [
        "!git clone https://github.com/ansarisam/models.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 34141, done.\u001b[K\n",
            "remote: Total 34141 (delta 0), reused 0 (delta 0), pack-reused 34141\u001b[K\n",
            "Receiving objects: 100% (34141/34141), 518.53 MiB | 35.77 MiB/s, done.\n",
            "Resolving deltas: 100% (22322/22322), done.\n",
            "Checking out files: 100% (3011/3011), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avl3MWYWqVD7"
      },
      "source": [
        "4. Compile the models project using the Protobuf compiler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGBnlTtrpJ-J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5019ae34-6afc-4551-b497-c8070a209f30"
      },
      "source": [
        "%%shell\n",
        "cd models/research\n",
        "protoc object_detection/protos/*.proto --python_out=."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFm20U3C1vbV"
      },
      "source": [
        "5. Set the following environment variables.\n",
        "\n",
        "https://stackoverflow.com/questions/53306150/setting-environment-variables-in-google-colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVxD5iV37_ZJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df0de841-c4bb-493c-ca52-e7f312b3b440"
      },
      "source": [
        "%env PYTHONPATH=$PYTHONPATH:/content/models/research/object_detection\n",
        "%env PYTHONPATH=$PYTHONPATH:/content/models/research\n",
        "%env PYTHONPATH=$PYTHONPATH:/content/models/research/slim"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: PYTHONPATH=$PYTHONPATH:/content/models/research/object_detection\n",
            "env: PYTHONPATH=$PYTHONPATH:/content/models/research\n",
            "env: PYTHONPATH=$PYTHONPATH:/content/models/research/slim\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6HEUxT6226v"
      },
      "source": [
        "6. Build and install the research project that we just built using\n",
        "Protobuf."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VS1Q3N0h22CG"
      },
      "source": [
        "%%shell\n",
        "cd /content/models/research\n",
        "\n",
        "python setup.py build\n",
        "python setup.py install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQr-l6KF4VY1"
      },
      "source": [
        "If the command successfully runs, it should print, at the end, something like this:\n",
        "\n",
        "`Finished processing dependencies for object-detection==0.1`\n",
        "\n",
        "We are all set with the environment preparation and ready to write code to detect objects in images. We will use the exported model that we downloaded from Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8Yvi6zv4m6U"
      },
      "source": [
        "## Code for Object Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c58VaTVz4nmt"
      },
      "source": [
        "Now we are ready to write code that does object detection in images and draws bounding boxes around them. To keep the code simple and easy to understand, we have divided it into the following parts:\n",
        "\n",
        "- **Configuration and initialization**: In this section of the code, we\n",
        "initialize the model path, image input, and output directories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37X-PYv_sJ-x"
      },
      "source": [
        "%%shell\n",
        "\n",
        "#copy object_detection to root due to set path not working\n",
        "cp -r models/research/object_detection .\n",
        "\n",
        "# donwload images for using prediction\n",
        "wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
        "tar -xvf images.tar.gz\n",
        "\n",
        "# unzip trained model\n",
        "unzip trained_models.zip \n",
        "\n",
        "mkdir output_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyoEXSwT3Awo"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "# Import the object detection module.\n",
        "from object_detection.utils import ops as utils_ops\n",
        "from object_detection.utils import label_map_util"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJqy_uK35VMk"
      },
      "source": [
        "# initializes the gfile in the TensorFlow2 compatibility mode.The gfile provides I/O functionality in TensorFlow.\n",
        "tf.gfile = tf.io.gfile\n",
        "\n",
        "# initializes the directory path where our object detection trained model is located.\n",
        "model_path = \"trained_models/final_model\"\n",
        "\n",
        "# initializes the mapping file path. We set the same JSON formatted file containing the class ID and class name mapping that we used for the training.\n",
        "labels_path = \"models/research/object_detection/data/pet_label_map.pbtxt\"\n",
        "\n",
        "# the input directory path containing images in which objects need to be detected.\n",
        "image_dir = \"images\"\n",
        "# defines the pattern of file names in the input image path. If you want to load all files from the directory, use *.*.\n",
        "image_file_pattern = \"*.jpg\"\n",
        "# the output directory path where the images with bounding boxes around the detected objects will be saved.\n",
        "output_path=\"output_dir\"\n",
        "\n",
        "# create iterable path objects that we will iterate through to read images one by one and detect objects in each of them.\n",
        "PATH_TO_IMAGES_DIR = pathlib.Path(image_dir)\n",
        "IMAGE_PATHS = sorted(list(PATH_TO_IMAGES_DIR.glob(image_file_pattern)))\n",
        "\n",
        "# List of the strings that is used to add correct label for each box.\n",
        "category_index = label_map_util.create_category_index_from_labelmap(labels_path, use_display_name=True)\n",
        "# assigned the number of classes to the class_num variable.\n",
        "class_num = len(category_index)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoKRdLv0t0CU"
      },
      "source": [
        "Let's initialize a color table that we will use when drawing bounding boxes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_o7tbnsty6-"
      },
      "source": [
        "# Creating a Color Table Based on the Number of Object Classes\n",
        "def get_color_table(class_num, seed=0):\n",
        "  random.seed(seed)\n",
        "  color_table = {}\n",
        "  for i in range(class_num):\n",
        "    color_table[i] = [random.randint(0, 255) for _ in range(3)]\n",
        "  \n",
        "  return color_table\n",
        "\n",
        "colortable = get_color_table(class_num)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIGYQoapus-R"
      },
      "source": [
        "- **Loading the Model**: Create a model object by loading the trained model.We will use this model object to predict the objects and bounding boxes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHucDlLLujOt"
      },
      "source": [
        "# Model preparation and loading the model from the disk\n",
        "def load_model(model_path):\n",
        "  model_dir = pathlib.Path(model_path) / \"saved_model\"\n",
        "  model = tf.saved_model.load(str(model_dir))\n",
        "  model = model.signatures[\"serving_default\"]\n",
        "\n",
        "  return model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeMjFt-5v2nV"
      },
      "source": [
        "- **Predicting Objects and Bounding Boxes and Organizing the Output**\n",
        "\n",
        "We run the prediction and construct the output in a usable form. We have written a function called run_inference_for_single_image() that takes two arguments: the model object and image NumPy. This function returns a Python dictionary. The output dictionary contains the following key pairs:\n",
        "\n",
        "1. detection_boxes, which is a 2D array consisting of the four corners\n",
        "of bounding boxes.\n",
        "2. detection_scores, which is a 1D array of scores associated with\n",
        "each bounding box.\n",
        "3. detection_classes, which is a 1D array of integer representation\n",
        "of the object class-index associated with each bounding box.\n",
        "4. num_detections, which is a scalar that indicates the number of\n",
        "predicted object classes.\n",
        "\n",
        "The TensorFlow model object takes a batch of image tensors to predict the object classes and bounding boxes around them. So we converts the image NumPy into a tensor. Since we are processing one image at a time and the model object takes a batch, we need to convert our image tensor into a batch of images.\n",
        "\n",
        "The tf.newaxis expression is used to increase the dimension of an existing array by 1, when used once. Thus, a 1D array will become a 2D array. A 2D array will become a 3D array. And so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIyarti4w2nY"
      },
      "source": [
        "# Predict objects and bounding boxes and format the result\n",
        "def run_inference_for_single_image(model, image):\n",
        "  #  The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
        "  input_tensor = tf.convert_to_tensor(image)\n",
        "  # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
        "  input_tensor = input_tensor[tf.newaxis, ...]\n",
        "\n",
        "  # Run prediction from the model and predicts the object classes, bounding boxes, and associated scores.\n",
        "  output_dict = model(input_tensor)\n",
        "\n",
        "  # Input to model is a tensor, so the output is also a tensor\n",
        "  # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
        "  # We're only interested in the first num_detections.\n",
        "  num_detections = int(output_dict.pop(\"num_detections\"))\n",
        "  output_dict = {\n",
        "      key: value[0, :num_detections].numpy() for key, value in output_dict.items()\n",
        "  }\n",
        "  output_dict[\"num_detections\"] = num_detections\n",
        "\n",
        "  # detection_classes should be ints.\n",
        "  output_dict[\"detection_classes\"] = output_dict[\"detection_classes\"].astype(np.int64)\n",
        "\n",
        "  # Handle models with masks: this is applicable only for a Mask R-CNN when masks need to be predicted. For all other predictors, these lines may be omitted.\n",
        "  if \"detection_masks\" in output_dict:\n",
        "    # Reframe the the bbox mask to the image size.\n",
        "    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "        output_dict[\"detection_masks\"],\n",
        "        output_dict[\"detection_boxes\"],\n",
        "        image.shape[0], image.shape[1]\n",
        "    )\n",
        "    detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
        "    output_dict[\"detection_masks_reframed\"] = detection_masks_reframed.numpy()\n",
        "\n",
        "  \"\"\"\n",
        "  returns the output dictionary, which consists of coordinates of detected bounding boxes, object classes, scores, and number of\n",
        "  detections. In the case of a Mask R-CNN, it also includes object masks.\n",
        "  \"\"\"\n",
        "  return output_dict"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ5zZQ-v4lrB"
      },
      "source": [
        "- **Drawing Bounding Boxes Around Detected Objects in Input Images**\n",
        "\n",
        "We will now write code to infer the output, draw bounding boxes around detected objects, and store the result. It will draws bounding boxes around each\n",
        "detected object in the image. It also labels the objects with class\n",
        "names and scores and finally saves the result to the output directory\n",
        "location."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC51rYJw3Y_n"
      },
      "source": [
        "def infer_object(model, image_path):\n",
        "  # Read the image using openCV and create an image numpy\n",
        "  # The final output image with boxes and labels on it.\n",
        "  imagename = os.path.basename(image_path)\n",
        "  image_np = cv2.imread(os.path.abspath(image_path))\n",
        "\n",
        "  # Actual detection.\n",
        "  output_dict = run_inference_for_single_image(model, image_np)\n",
        "\n",
        "  # Visualization of the results of a detection.\n",
        "  for i in range(output_dict[\"detection_classes\"].size):\n",
        "    box = output_dict[\"detection_boxes\"][i]\n",
        "    classes = output_dict[\"detection_classes\"][i]\n",
        "    scores = output_dict[\"detection_scores\"][i]\n",
        "\n",
        "    if scores > 0.5:\n",
        "      h = image_np.shape[0]\n",
        "      w = image_np.shape[1]\n",
        "      classname = category_index[classes][\"name\"]\n",
        "      classid = category_index[classes][\"id\"]\n",
        "\n",
        "      # Draw bounding boxes\n",
        "      cv2.rectangle(image_np, (int(box[1] * w), int(box[0] * h)), (int(box[3] * w), int(box[2] * h)), colortable[classid], 2)\n",
        "\n",
        "      # Write the class name on top of the bounding box\n",
        "      font = cv2.FONT_HERSHEY_COMPLEX_SMALL\n",
        "      size = cv2.getTextSize(str(classname) + \":\" + str(scores), font, 0.75, 1)[0][0]\n",
        "\n",
        "      cv2.rectangle(image_np,(int(box[1] * w), int(box[0] * h-20)), ((int(box[1] * w)+size+5), int(box[0] * h)), colortable[classid],-1)\n",
        "      cv2.putText(image_np, str(classname) + \":\" + str(scores), (int(box[1] * w), int(box[0] * h)-5), font, 0.75, (0,0,0), 1, 1)\n",
        "    else:\n",
        "      break\n",
        "  # Save the result image with bounding boxes and class labels in file system\n",
        "  cv2.imwrite(output_path + \"/\" + imagename, image_np)\n",
        "  # cv2.imshow(imagename, image_np)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iACQpEad8QkD"
      },
      "source": [
        "Now that we have all the right settings and functions defined, we need to call them to trigger the detection process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7jWUMv48PvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "889488f9-8fc4-4adf-cbff-2365106c7404"
      },
      "source": [
        "# Obtain the model object\n",
        "detection_model = load_model(model_path)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAFuB1QQ9dme"
      },
      "source": [
        "The function infer_object() is invoked for each image, and the final output with\n",
        "bounding boxes around the detected objects are saved in the output directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0xJx5HS8gx9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "bcdb2555-ce38-4a0b-fa32-115fcfcff364"
      },
      "source": [
        "# For each image, call the prediction\n",
        "for image_path in IMAGE_PATHS:\n",
        "  #print(image_path)\n",
        "  infer_object(detection_model, image_path)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-9c719d701bb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mIMAGE_PATHS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m#print(image_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0minfer_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetection_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-941c848e4962>\u001b[0m in \u001b[0;36minfer_object\u001b[0;34m(model, image_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m# Actual detection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0moutput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_inference_for_single_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# Visualization of the results of a detection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-13e24db70efa>\u001b[0m in \u001b[0;36mrun_inference_for_single_image\u001b[0;34m(model, image)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_inference_for_single_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m#  The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;31m# The model expects a batch of images, so add an axis with `tf.newaxis`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1380\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    336\u001b[0m                                          as_ref=False):\n\u001b[1;32m    337\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    262\u001b[0m   \"\"\"\n\u001b[1;32m    263\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 264\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    273\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH17eJwsLW2C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}