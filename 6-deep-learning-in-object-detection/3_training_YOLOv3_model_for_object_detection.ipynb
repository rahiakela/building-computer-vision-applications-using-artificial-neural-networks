{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3-training-YOLOv3-model-for-object-detection.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOk8S7t8QuCsm2ShWWooSHr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/building-computer-vision-applications-using-artificial-neural-networks/blob/master/6-deep-learning-in-object-detection/3_training_YOLOv3_model_for_object_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sikCBUVyiD79"
      },
      "source": [
        "# Training a YOLOv3 Model for Object Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LJR1KskiRLk"
      },
      "source": [
        "YOLOv3 is the youngest of all the object detection algorithms.It has not made it to the TensorFlow object detection API yet.YOLOv3 uses the Darknet-53 architecture to train the model.\n",
        "\n",
        "We will use the official API and weights of the pretrained model to perform transfer learning of our YOLOv3 model from the same Oxford-IIIT Pet dataset that we used in the previous SSD model. We will run the training on Google Colab and use a GPU hardware accelerator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7JIoxCfiq2f"
      },
      "source": [
        "## Installing the Darknet Framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyqDop9eirtJ"
      },
      "source": [
        "Darknet is an open source neural network framework written in C and CUDA that runs on both CPUs and GPUs. First, clone the Darknet GitHub repository and then build the source."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O00SkYDYiwBj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8775f4eb-7941-4b56-edf7-c4c2c6298586"
      },
      "source": [
        "%%shell\n",
        "\n",
        "git clone https://github.com/ansarisam/darknet.git\n",
        "# Official repository\n",
        "#git clone https://github.com/pjreddie/darknet.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'darknet'...\n",
            "remote: Enumerating objects: 5912, done.\u001b[K\n",
            "remote: Total 5912 (delta 0), reused 0 (delta 0), pack-reused 5912\u001b[K\n",
            "Receiving objects: 100% (5912/5912), 6.34 MiB | 29.66 MiB/s, done.\n",
            "Resolving deltas: 100% (3922/3922), done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdImVaJakN1c"
      },
      "source": [
        "After the repository is cloned, expand the file browser, navigate to the darknet\n",
        "directory, and download the Makefile to your local computer. Edit the Makefile\n",
        "(highlighted in bold letters) and change GPU=1 and OPENCV=1, as shown here:\n",
        "\n",
        "```c\n",
        "GPU=1\n",
        "CUDNN=0\n",
        "OPENCV=1\n",
        "OPENMP=0\n",
        "DEBUG=0\n",
        "```\n",
        "\n",
        "Make sure no other change is made to the Makefile, or you may have trouble\n",
        "building your Darknet code.\n",
        "\n",
        "Now we are ready to build the Darknet framework."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68Jg-uA4kFCG"
      },
      "source": [
        "# Running the make Command to Build Darknet\n",
        "%%shell\n",
        "cd darknet/\n",
        "make"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL2t2rcklT1x"
      },
      "source": [
        "After the build process successfully completes, run the below command to test your installation. \n",
        "\n",
        "It should print `usage: ./darknet <function>` if the installation is successful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLdsENvUlGCO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f2754c5-7ba0-4510-a5a6-e96686fc6ce1"
      },
      "source": [
        "# Testing the Darknet Installation\n",
        "%%shell\n",
        "cd darknet\n",
        "./darknet"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: ./darknet <function>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFQ4oLcgmEyM"
      },
      "source": [
        "## Downloading Pre-trained Convolutional Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxir3Q6WmFh9"
      },
      "source": [
        "Let's downloads pre-trained weights of the COCO dataset trained on the\n",
        "Darknet-53 framework."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlATwoGflnuM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64de3db8-6f5f-40ae-b1f9-5ead50b01e55"
      },
      "source": [
        "# Downloading Pre-trained Darknet-53 Weights\n",
        "%%shell\n",
        "mkdir pretrained\n",
        "cd pretrained\n",
        "wget https://pjreddie.com/media/files/darknet53.conv.74"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-01 08:59:58--  https://pjreddie.com/media/files/darknet53.conv.74\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 162482580 (155M) [application/octet-stream]\n",
            "Saving to: ‘darknet53.conv.74’\n",
            "\n",
            "darknet53.conv.74   100%[===================>] 154.96M   174KB/s    in 15m 51s \n",
            "\n",
            "2020-12-01 09:15:51 (167 KB/s) - ‘darknet53.conv.74’ saved [162482580/162482580]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_PjDkGAmW7n"
      },
      "source": [
        "## Downloading an Annotated Oxford-IIIT Pet Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da6fdSbzmZXN"
      },
      "source": [
        "Let's downloads the pet dataset with both the images and annotations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFB0ps4UmUS4"
      },
      "source": [
        "# Downloading the Pet Dataset Images and Annotations\n",
        "%%shell\n",
        "\n",
        "mkdir petdata\n",
        "cd petdata\n",
        "\n",
        "wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
        "wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
        "\n",
        "tar -xvf images.tar.gz\n",
        "tar -xvf annotations.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jX-1J69m-2Q"
      },
      "source": [
        "The images directory contains a few files with the extension .mat, which\n",
        "causes the training to break. So need to remove these .mat files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhcbqGP-nDJi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8660d587-757e-42cd-c175-2db73d1d2388"
      },
      "source": [
        "# Deleting the Invalid File Extension .mat\n",
        "%%shell\n",
        "\n",
        "cd /content/petdata/images\n",
        "rm *.mat"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgK3rcFpnNSm"
      },
      "source": [
        "## Preparing the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFQkqeVOnRaB"
      },
      "source": [
        "The YOLOv3 training API expects the dataset to have a certain format and directory structure. The pet data that we downloaded has two subdirectories:\n",
        "\n",
        "- **images** and \n",
        "- **annotations**. \n",
        "\n",
        "The images directory contains all the labeled images that we will use for\n",
        "training and testing. The annotations directory contains annotation files in XML format, one XML file per image.\n",
        "\n",
        "YOLOv3 expects the following files:\n",
        "\n",
        "- **train.txt**: This file contains the absolute path of images—one image path\n",
        "per line—that will be used for training.\n",
        "- **test.txt**: This file contains the absolute path of images—one image path\n",
        "per line—that will be used for testing.\n",
        "- **class.data**: This file contains a list of names of the object classes—one\n",
        "name per line.\n",
        "- **labels**: This directory is in the same location where train.txt and test.txt are located. This labels directory contains annotation files, one file per image. The file name in this directory must be the same as the image file name, except that it has the extension .txt.\n",
        "\n",
        "For example, if the image file name is `Abyssinian_1.jpg`, the annotation file name in the labels directory must be `Abyssinian_1.txt`. Each annotation text file must contain the annotated bounding box and object class in one single line in the following format:\n",
        "\n",
        "```python\n",
        "<object-class> <x_center> <y_center> <width> <height>\n",
        "```\n",
        "\n",
        "where\n",
        "- `<object-class>` is the integer class index of the object, from 0 to (num_\n",
        "class-1).\n",
        "- `<x_center> and <y_center>` are float values representing the center of\n",
        "the bounding boxes relative to the image height and width.\n",
        "- `<width> <height>` are the width and height of bounding boxes relative\n",
        "to the image height and width.\n",
        "\n",
        "Note that the entries in this file are separated by blank spaces and not\n",
        "by commas or any other delimiters.\n",
        "\n",
        "An example entry of the annotation text file is as follows (ensure the fields are separated by white space and not comma or any other delimiter.):\n",
        "\n",
        "```python\n",
        "10 0.63 0.28500000000000003 0.28500000000000003 0.215\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPUrEmvll8VP"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAvao7J_uqyQ"
      },
      "source": [
        "# Converting Image Annotations from XML to TXT\n",
        "def xml_to_csv(path, img_path, label_path):\n",
        "  if not os.path.exists(label_path):\n",
        "    os.makedirs(label_path)\n",
        "\n",
        "  class_list = []\n",
        "  for xml_file in glob.glob(path + \"/*.xml\"):\n",
        "    xml_list = []\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "    for member in root.findall(\"object\"):\n",
        "      imagename = str(root.find(\"filename\").text)\n",
        "      print(\"image \", imagename)\n",
        "      index = int(imagename.rfind(\"_\"))\n",
        "      print(\"index: \", index)\n",
        "      classname = imagename[0:index]\n",
        "\n",
        "      class_index = 0\n",
        "      if (class_list.count(classname) > 0):\n",
        "        class_index = class_list.index(classname)\n",
        "      else:\n",
        "        class_list.append(classname)\n",
        "        class_index = class_list.index(classname)\n",
        "\n",
        "      print(\"width: \", root.find(\"size\").find(\"width\").text)\n",
        "      print(\"height: \", root.find(\"size\").find(\"height\").text)\n",
        "      print(\"minx: \", member[4][0].text)\n",
        "      print(\"ymin:\", member[4][1].text)\n",
        "      print(\"maxx: \", member[4][2].text)\n",
        "      print(\"maxy: \", member[4][3].text)\n",
        "      w = float(root.find(\"size\").find(\"width\").text)\n",
        "      h = float(root.find(\"size\").find(\"height\").text)\n",
        "      dw = 1.0 / w\n",
        "      dh = 1.0 / h\n",
        "      x = (float(member[4][0].text) + float(member[4][2].text)) / 2.0 - 1\n",
        "      y = (float(member[4][1].text) + float(member[4][3].text)) / 2.0 - 1\n",
        "      w = float(member[4][2].text) - float(member[4][0].text)\n",
        "      h = float(member[4][3].text) - float(member[4][1].text)\n",
        "      x = x * dw\n",
        "      w = w * dw\n",
        "      y = y * dh\n",
        "      h = h * dh\n",
        "\n",
        "      value = (class_index, x, y, y, h)\n",
        "      print(\"The line value is: \", value)\n",
        "      print(\"csv file name: \", os.path.join(label_path, imagename.rsplit('.', 1)[0] + '.txt'))\n",
        "      xml_list.append(value)\n",
        "      \n",
        "      df = pd.DataFrame(xml_list)\n",
        "      df.to_csv(os.path.join(label_path, imagename.rsplit(\".\", 1)[0] + \".txt\"), index=None, header=False, sep=\" \")\n",
        "\n",
        "  class_df = pd.DataFrame(class_list)\n",
        "\n",
        "  return class_df"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjGOaf5ztNQ4"
      },
      "source": [
        "def create_training_and_test(image_dir, label_dir):\n",
        "  file_list = []\n",
        "  for img in glob.glob(image_dir + \"/*\"):\n",
        "    print(os.path.abspath(img))\n",
        "\n",
        "    imagefile = os.path.basename(img)\n",
        "    textfile = imagefile.rsplit(\".\", 1)[0] + \".txt\"\n",
        "\n",
        "    if not os.path.isfile(label_dir + \"/\" + textfile):\n",
        "      print(\"delete image file \", img)\n",
        "      os.remove(img)\n",
        "      continue\n",
        "    file_list.append(os.path.abspath(img))\n",
        "\n",
        "  file_df = pd.DataFrame(file_list)\n",
        "  train = file_df.sample(frac=0.7, random_state=10)\n",
        "  test = file_df.drop(train.index)\n",
        "  train.to_csv(\"petdata/train.txt\", index=None, header=False)\n",
        "  test.to_csv(\"petdata/test.txt\", index=None, header=False)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ei8LP0avEvG"
      },
      "source": [
        "img_dir = \"petdata/images\"\n",
        "label_dir = \"petdata/labels\"\n",
        "\n",
        "xml_path = os.path.join(os.getcwd(), \"petdata/annotations/xmls\")\n",
        "img_path = os.path.join(os.getcwd(), img_dir)\n",
        "label_path = os.path.join(os.getcwd(), label_dir)\n",
        "\n",
        "class_df = xml_to_csv(xml_path, img_path, label_path)\n",
        "class_df.to_csv(\"petdata/class.data\", index=None, header=False)\n",
        "create_training_and_test(img_dir, label_dir)\n",
        "print(\"Successfully converted xml to csv.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MH5bARMyvXf"
      },
      "source": [
        "## Configuring the Training Input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqVYlYiOyy1m"
      },
      "source": [
        "We need a configuration file that has the path information for the training and test sets.\n",
        "\n",
        "The format of the config file is as follows:\n",
        "\n",
        "```python\n",
        "classes= 37\n",
        "train = /content/petdata/train.txt\n",
        "valid = /content/petdata/test.txt\n",
        "names = /content/petdata/class.data\n",
        "backup = /content/yolov3_model\n",
        "```\n",
        "\n",
        "where the classes variable takes the number of object classes our training images have (37 pet classes in our example), the train and valid variables take the path to the training and validation lists that we created earlier, names takes the path to the file containing class names, and the backup variable points to the directory path where the trained YOLO model will be saved.\n",
        "\n",
        "Save this text file and give it a name with a .cfg extension. In our case, we save this file as pet_input.cfg. We will then upload this file to Colab in the directory path /content/darknet/cfg."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7lLmYoy6dgF",
        "outputId": "1105e663-e107-4e40-d42e-2ad3a6a824a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%shell\n",
        "\n",
        "# download update pet_input.cfg from github\n",
        "wget https://raw.githubusercontent.com/rahiakela/building-computer-vision-applications-using-artificial-neural-networks/master/6-deep-learning-in-object-detection/pet_input.cfg\n",
        "\n",
        "# copy dowloaded file to /content/darknet/cfg\n",
        "cp pet_input.cfg /content/darknet/cfg"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-01 10:49:42--  https://raw.githubusercontent.com/rahiakela/building-computer-vision-applications-using-artificial-neural-networks/master/6-deep-learning-in-object-detection/pet_input.cfg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 148 [text/plain]\n",
            "Saving to: ‘pet_input.cfg’\n",
            "\n",
            "pet_input.cfg       100%[===================>]     148  --.-KB/s    in 0s      \n",
            "\n",
            "2020-12-01 10:49:42 (4.34 MB/s) - ‘pet_input.cfg’ saved [148/148]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKhX6D-j0sFH"
      },
      "source": [
        "## Configuring the Darknet Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bENzaH0Y0s-w"
      },
      "source": [
        "Download the sample network config file from `/content/darknet/cfg/yolov3-voc.cfg` from Colab and save it in your local computer. You may rename this file to something relevant to your dataset. For example, we have renamed it to `yolov3-pet.cfg` for this exercise.\n",
        "\n",
        "We will edit this file to match our data. The most important part of the file that we are going to edit is the yolo layer.\n",
        "\n",
        "Search for the section `[yolo]` in the config file. There should be three yolo layers. We will edit the number of object classes, which is 37 in our case. In all three places, we will change the number of classes to 37.\n",
        "\n",
        "In addition, we will change the filters values in the convolutional layer just before the yolo layer in all three places. The value of filters in the convolutional layer before the yolo layer is determined by the following formula:\n",
        "\n",
        "```python\n",
        "filter = num/3 * (num_class+5)\n",
        "Filter = (9/3) * (37 + 5) = 126\n",
        "```\n",
        "\n",
        "Make sure you changed the classes and filters values at three places in the config file.\n",
        "\n",
        "Other parameters that we will edit are as follows:\n",
        "\n",
        "- **width=416**, which is the width of the input image. All images will\n",
        "be resized to this width.\n",
        "- **height=416**, which is the height of the input image. All images will\n",
        "be resized to this height.\n",
        "- **batch=64**, which indicates how frequently we want weights to be\n",
        "updated.\n",
        "- **subdivisions=16**, which indicates how many examples will be\n",
        "loaded in memory if the GPU does not have large enough memory\n",
        "to load the data examples equal to the batch size. If you see an “out\n",
        "of memory” exception when you execute the training, tune this\n",
        "number and gradually decrease it until you see no memory error.\n",
        "- **max_batches=74000**, which indicates how many batches the training\n",
        "should run. If you set it too high, the training may take a long time\n",
        "to complete. If it is too low, the network will not learn enough.\n",
        "Practically, it has been established that the max_batch size should be\n",
        "2,000 times the number of classes. In our case, we have 37 classes, so\n",
        "the max_batch value should be 2,000×37 = 74,000. If you have only\n",
        "one class, set the max_batches value to a minimum of 4,000.\n",
        "\n",
        "Save the config file and then upload it to the cfg directory path: `/\n",
        "content/darknet/cfg`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6mgCL3jv3jk",
        "outputId": "82b66b9f-6714-42b1-9080-979c579bc204",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%shell\n",
        "\n",
        "# download update yolov3-pet.cfg from github\n",
        "wget https://raw.githubusercontent.com/rahiakela/building-computer-vision-applications-using-artificial-neural-networks/master/6-deep-learning-in-object-detection/yolov3-pet.cfg\n",
        "\n",
        "# copy dowloaded file to /content/darknet/cfg\n",
        "cp yolov3-pet.cfg /content/darknet/cfg"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-01 11:22:47--  https://raw.githubusercontent.com/rahiakela/building-computer-vision-applications-using-artificial-neural-networks/master/6-deep-learning-in-object-detection/yolov3-pet.cfg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8337 (8.1K) [text/plain]\n",
            "Saving to: ‘yolov3-pet.cfg.2’\n",
            "\n",
            "\ryolov3-pet.cfg.2      0%[                    ]       0  --.-KB/s               \ryolov3-pet.cfg.2    100%[===================>]   8.14K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-12-01 11:22:47 (85.9 MB/s) - ‘yolov3-pet.cfg.2’ saved [8337/8337]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO2GlNO47ic8"
      },
      "source": [
        "## Training a YOLOv3 Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e1ia2qo7jPV"
      },
      "source": [
        "Now let's execute the YOLOv3 training by passing the parameters to the training are the paths to `pet_input.cfg`, `yolov3-pet.cfg`, and the pre-trained darknet model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkXwLtCs7VSW",
        "outputId": "e35a2767-dc59-4b06-b1e9-756b4f8f2b14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%shell\n",
        "\n",
        "cd darknet/\n",
        "./darknet detector train cfg/pet_input.cfg cfg/yolov3-pet.cfg /content/pretrained/darknet53.conv.74"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Region 82 Avg IOU: -nan, Class: -nan, Obj: -nan, No Obj: 0.540412, .5R: -nan, .75R: -nan,  count: 0\n",
            "Region 94 Avg IOU: 0.054834, Class: 0.494438, Obj: 0.664120, No Obj: 0.513917, .5R: 0.000000, .75R: 0.000000,  count: 2\n",
            "Region 106 Avg IOU: -nan, Class: -nan, Obj: -nan, No Obj: 0.469831, .5R: -nan, .75R: -nan,  count: 0\n",
            "Region 82 Avg IOU: -nan, Class: -nan, Obj: -nan, No Obj: 0.538877, .5R: -nan, .75R: -nan,  count: 0\n",
            "Region 94 Avg IOU: 0.251527, Class: 0.298048, Obj: 0.152079, No Obj: 0.512447, .5R: 0.000000, .75R: 0.000000,  count: 1\n",
            "Region 106 Avg IOU: -nan, Class: -nan, Obj: -nan, No Obj: 0.468161, .5R: -nan, .75R: -nan,  count: 0\n",
            "Region 82 Avg IOU: -nan, Class: -nan, Obj: -nan, No Obj: 0.541067, .5R: -nan, .75R: -nan,  count: 0\n",
            "Region 94 Avg IOU: 0.313674, Class: 0.000000, Obj: 0.164733, No Obj: 0.512926, .5R: 0.000000, .75R: 0.000000,  count: 1\n",
            "Region 106 Avg IOU: -nan, Class: -nan, Obj: -nan, No Obj: 0.468786, .5R: -nan, .75R: -nan,  count: 0\n",
            "Region 82 Avg IOU: -nan, Class: -nan, Obj: -nan, No Obj: 0.540215, .5R: -nan, .75R: -nan,  count: 0\n",
            "Region 94 Avg IOU: -nan, Class: -nan, Obj: -nan, No Obj: 0.514054, .5R: -nan, .75R: -nan,  count: 0\n",
            "Region 106 Avg IOU: -nan, Class: -nan, Obj: -nan, No Obj: 0.469641, .5R: -nan, .75R: -nan,  count: 0\n",
            "Region 82 Avg IOU: -nan, Class: -nan, Obj: -nan, No Obj: 0.538281, .5R: -nan, .75R: -nan,  count: 0\n",
            "Region 94 Avg IOU: -nan, Class: -nan, Obj: -nan, No Obj: 0.511662, .5R: -nan, .75R: -nan,  count: 0\n",
            "Region 106 Avg IOU: -nan, Class: -nan, Obj: -nan, No Obj: 0.471319, .5R: -nan, .75R: -nan,  count: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1mXxKt0LWDt"
      },
      "source": [
        "Notice the last three lines, which are printed at the end when the training is\n",
        "completely done. It shows the location where the checkpoints, intermediate weights, and final weights are saved.\n",
        "\n",
        "You should copy the entire directory containing the final model to your private\n",
        "Google Drive so that you could use the trained model in your applications.\n",
        "\n",
        "While the training is on, the console prints a lot of information, which is displayed in the web browser. After a while, the web browser becomes unresponsive. Clearing the console output may be a good idea to prevent the browser from getting killed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxeCY1LCMPTK"
      },
      "source": [
        "## How Long the Training Should Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcM9M7zQMQL3"
      },
      "source": [
        "Typically the training should run for at least 2,000 iterations per class, but not less than 4,000 iterations in total. In our example with a pet dataset, we have 37 classes. That means we should set max_batches to 74000.\n",
        "\n",
        "Observe the output while the training is going on, and notice the losses after each iteration. If the loss stabilizes and does not change over batches, we should consider stopping the training. Ideally, the loss should be close to zero. However, for most practical purposes, our goal should be to have losses stabilized below 0.05."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy3Nnb51Mk-O"
      },
      "source": [
        "## Final Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz5yc8cdMlzg"
      },
      "source": [
        "After the network finishes learning, the final YOLOv3 model will be saved in the directory `/content/yolov3_model`. The name of the model file will be `yolov3-pet_final.weights`.\n",
        "\n",
        "Download this model or save it to your private Google Drive folder, because Google Colab deletes all your files when the session expires. We will use this model in object detection in real time, both in images and in videos."
      ]
    }
  ]
}